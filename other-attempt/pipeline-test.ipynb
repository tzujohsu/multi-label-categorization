{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "from abc import ABC, abstractmethod\n",
    "from src.ingest_data import *\n",
    "import pandas as pd\n",
    "import logging\n",
    "from src.filter_text import *\n",
    "from src.feature_engineering import *\n",
    "from src.data_splitter import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"data/archive.zip\"\n",
    "\n",
    "# Determine the file extension\n",
    "file_extension = os.path.splitext(file_path)[1]\n",
    "\n",
    "# Get the appropriate DataIngestor\n",
    "data_ingestor = DataIngestorFactory.get_data_ingestor(file_extension)\n",
    "\n",
    "# Ingest the data and load it into a DataFrame\n",
    "df = data_ingestor.ingest(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 16:38:55,679 - INFO - Executing text filtering handling strategy.\n",
      "2025-02-19 16:38:55,680 - INFO - Dropping news with word count (length) <= 25\n",
      "2025-02-19 16:38:56,144 - INFO - News Filtered by length >= 25.\n",
      "/Users/kiyas/Documents/Programming_Practice/portfolio/multi-label-news-categorization/src/filter_text.py:53: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned.drop(columns=['content_length'], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "text_filtering_handler = FilterTextHandler(FixDropTextStrategy(length=25))\n",
    "df_cleaned = text_filtering_handler.handle_text_filtering(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 16:38:56,866 - INFO - Applying text preprocessing (basic cleaning) to text:\n",
      "2025-02-19 16:39:05,695 - INFO - Basic Preprocessing completed.\n",
      "2025-02-19 16:39:05,700 - INFO - Applying stopword removing to text:\n",
      "2025-02-19 16:39:14,852 - INFO - Stopword Removing completed.\n",
      "2025-02-19 16:39:14,905 - INFO - Applying Stemming to text:\n",
      "2025-02-19 16:39:51,780 - INFO - Text Stemming completed.\n"
     ]
    }
   ],
   "source": [
    "basic_preprocess = FeatureEngineer(BasicPreprocessText(feature='content'))\n",
    "df = basic_preprocess.apply_feature_engineering(df_cleaned)\n",
    "\n",
    "stopword_remover = FeatureEngineer(RemoveStopwords(feature='content'))\n",
    "df = stopword_remover.apply_feature_engineering(df)\n",
    "\n",
    "text_stemming = FeatureEngineer(TextStemming(feature='content'))\n",
    "df = text_stemming.apply_feature_engineering(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 16:39:51,826 - INFO - Splitting data using the selected strategy.\n",
      "2025-02-19 16:39:51,830 - INFO - Performing simple train-test split.\n",
      "2025-02-19 16:39:51,868 - INFO - Train-test split completed.\n"
     ]
    }
   ],
   "source": [
    "data_splitter = DataSplitter(SimpleTrainTestSplitStrategy(test_size=0.2, random_state=42))\n",
    "X_train, X_test, y_train, y_test = data_splitter.split(df, target_column=df.columns[17:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OneVsRestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, roc_curve, auc, hamming_loss\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.calibration import CalibratedClassifierCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_importance(pipeline):\n",
    "    '''\n",
    "    Extract feature importances from pipeline. \n",
    "    Since I am using CalibratedClassifierCV I will average the coefficients over calibrated classifiers.\n",
    "    \n",
    "    https://www.kaggle.com/code/kobakhit/eda-and-multi-label-classification-for-arxiv#Preprocess-data\n",
    "    '''\n",
    "    # average coefficients over all calibrated classifiers\n",
    "    coef_avg = 0\n",
    "    classifiers = pipeline[1].estimators_[0].calibrated_classifiers_\n",
    "    for i in classifiers:\n",
    "        coef_avg = coef_avg + i.estimator.coef_\n",
    "    coef_avg  = (coef_avg/len(classifiers)).tolist()[0]\n",
    "    # get feature names from tf-idf vectorizer\n",
    "    features = pipeline[0].get_feature_names_out()\n",
    "    # get 10 most important features\n",
    "    top_f = pd.DataFrame(list(zip(features,coef_avg)), columns = ['token','coef']) \\\n",
    "        .nlargest(10,'coef').to_dict(orient = 'records')\n",
    "    return top_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Processing crime, law and justice\n",
      "Accuracy : 0.9756 . Area under the ROC curve : 0.9792\n",
      "\n",
      "... Processing arts, culture, entertainment and media\n",
      "Accuracy : 0.9618 . Area under the ROC curve : 0.9825\n",
      "\n",
      "... Processing economy, business and finance\n",
      "Accuracy : 0.9586 . Area under the ROC curve : 0.972\n",
      "\n",
      "... Processing disaster, accident and emergency incident\n",
      "Accuracy : 0.9847 . Area under the ROC curve : 0.9878\n",
      "\n",
      "... Processing environment\n",
      "Accuracy : 0.9866 . Area under the ROC curve : 0.9921\n",
      "\n",
      "... Processing education\n",
      "Accuracy : 0.9847 . Area under the ROC curve : 0.9955\n",
      "\n",
      "... Processing health\n",
      "Accuracy : 0.9695 . Area under the ROC curve : 0.9898\n",
      "\n",
      "... Processing human interest\n",
      "Accuracy : 0.9546 . Area under the ROC curve : 0.9688\n",
      "\n",
      "... Processing lifestyle and leisure\n",
      "Accuracy : 0.988 . Area under the ROC curve : 0.9918\n",
      "\n",
      "... Processing politics\n",
      "Accuracy : 0.932 . Area under the ROC curve : 0.9659\n",
      "\n",
      "... Processing labour\n",
      "Accuracy : 0.9851 . Area under the ROC curve : 0.9929\n",
      "\n",
      "... Processing religion and belief\n",
      "Accuracy : 0.9807 . Area under the ROC curve : 0.9885\n",
      "\n",
      "... Processing science and technology\n",
      "Accuracy : 0.9389 . Area under the ROC curve : 0.9617\n",
      "\n",
      "... Processing society\n",
      "Accuracy : 0.9066 . Area under the ROC curve : 0.9366\n",
      "\n",
      "... Processing sport\n",
      "Accuracy : 0.9866 . Area under the ROC curve : 0.9969\n",
      "\n",
      "... Processing conflict, war and peace\n",
      "Accuracy : 0.9822 . Area under the ROC curve : 0.9958\n",
      "\n",
      "... Processing weather\n",
      "Accuracy : 0.996 . Area under the ROC curve : 0.9997\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classifier = CalibratedClassifierCV(LinearSVC()) \n",
    "categories = y_train.columns\n",
    "\n",
    "# for each category train the model and get accuracy, auc\n",
    "models = {}\n",
    "features = {}\n",
    "preds = {}\n",
    "for category in categories:\n",
    "    # give pipelines unique names. important!  \n",
    "    SVC_pipeline = Pipeline([\n",
    "                (f'tfidf_{category}', TfidfVectorizer()),\n",
    "                (f'clf_{category}', OneVsRestClassifier(classifier, n_jobs=1)),\n",
    "            ])\n",
    "    print('... Processing {}'.format(category))\n",
    "    # train the model using X_dtm & y\n",
    "    SVC_pipeline.fit(X_train['content'], y_train[category])\n",
    "    models[category] = SVC_pipeline\n",
    "    # compute the testing accuracy\n",
    "    prediction = SVC_pipeline.predict(X_test['content'])\n",
    "    preds[category] = prediction\n",
    "    accuracy = accuracy_score(y_test[category], prediction)\n",
    "    # compute auc\n",
    "    probas_ = SVC_pipeline.predict_proba(X_test['content'])\n",
    "    fpr, tpr, thresholds = roc_curve(y_test[category], probas_[:, 1])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    print(\"Accuracy : {} . Area under the ROC curve : {}\".format(round(accuracy,4), round(roc_auc,4)))\n",
    "    print()\n",
    "    # get most predictive features\n",
    "    features[category] = feature_importance(SVC_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crime, law and justice</th>\n",
       "      <th>arts, culture, entertainment and media</th>\n",
       "      <th>economy, business and finance</th>\n",
       "      <th>disaster, accident and emergency incident</th>\n",
       "      <th>environment</th>\n",
       "      <th>education</th>\n",
       "      <th>health</th>\n",
       "      <th>human interest</th>\n",
       "      <th>lifestyle and leisure</th>\n",
       "      <th>politics</th>\n",
       "      <th>labour</th>\n",
       "      <th>religion and belief</th>\n",
       "      <th>science and technology</th>\n",
       "      <th>society</th>\n",
       "      <th>sport</th>\n",
       "      <th>conflict, war and peace</th>\n",
       "      <th>weather</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>law</td>\n",
       "      <td>entertain</td>\n",
       "      <td>compani</td>\n",
       "      <td>fire</td>\n",
       "      <td>environment</td>\n",
       "      <td>educ</td>\n",
       "      <td>health</td>\n",
       "      <td>ceremoni</td>\n",
       "      <td>tattoo</td>\n",
       "      <td>polit</td>\n",
       "      <td>retir</td>\n",
       "      <td>religi</td>\n",
       "      <td>scienc</td>\n",
       "      <td>immigr</td>\n",
       "      <td>sport</td>\n",
       "      <td>protest</td>\n",
       "      <td>storm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sentenc</td>\n",
       "      <td>cultur</td>\n",
       "      <td>economi</td>\n",
       "      <td>emerg</td>\n",
       "      <td>climat</td>\n",
       "      <td>school</td>\n",
       "      <td>hospit</td>\n",
       "      <td>award</td>\n",
       "      <td>workout</td>\n",
       "      <td>trump</td>\n",
       "      <td>job</td>\n",
       "      <td>church</td>\n",
       "      <td>research</td>\n",
       "      <td>chariti</td>\n",
       "      <td>stadium</td>\n",
       "      <td>war</td>\n",
       "      <td>weather</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>murder</td>\n",
       "      <td>film</td>\n",
       "      <td>market</td>\n",
       "      <td>crash</td>\n",
       "      <td>pollut</td>\n",
       "      <td>student</td>\n",
       "      <td>diseas</td>\n",
       "      <td>plant</td>\n",
       "      <td>exercis</td>\n",
       "      <td>govern</td>\n",
       "      <td>employe</td>\n",
       "      <td>mosqu</td>\n",
       "      <td>math</td>\n",
       "      <td>wed</td>\n",
       "      <td>footbal</td>\n",
       "      <td>militari</td>\n",
       "      <td>cyclon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>polic</td>\n",
       "      <td>museum</td>\n",
       "      <td>econom</td>\n",
       "      <td>disast</td>\n",
       "      <td>insect</td>\n",
       "      <td>learn</td>\n",
       "      <td>treatment</td>\n",
       "      <td>birthday</td>\n",
       "      <td>garden</td>\n",
       "      <td>amnesti</td>\n",
       "      <td>worker</td>\n",
       "      <td>muslim</td>\n",
       "      <td>clinic</td>\n",
       "      <td>addict</td>\n",
       "      <td>athlet</td>\n",
       "      <td>coup</td>\n",
       "      <td>temperatur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>court</td>\n",
       "      <td>art</td>\n",
       "      <td>custom</td>\n",
       "      <td>incid</td>\n",
       "      <td>spill</td>\n",
       "      <td>teacher</td>\n",
       "      <td>nhs</td>\n",
       "      <td>dog</td>\n",
       "      <td>game</td>\n",
       "      <td>googl</td>\n",
       "      <td>union</td>\n",
       "      <td>christian</td>\n",
       "      <td>test</td>\n",
       "      <td>fan</td>\n",
       "      <td>bodybuild</td>\n",
       "      <td>terrorist</td>\n",
       "      <td>snow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>attorney</td>\n",
       "      <td>tradit</td>\n",
       "      <td>product</td>\n",
       "      <td>accid</td>\n",
       "      <td>speci</td>\n",
       "      <td>cours</td>\n",
       "      <td>medic</td>\n",
       "      <td>celebr</td>\n",
       "      <td>fit</td>\n",
       "      <td>elect</td>\n",
       "      <td>labour</td>\n",
       "      <td>belief</td>\n",
       "      <td>space</td>\n",
       "      <td>refuge</td>\n",
       "      <td>championship</td>\n",
       "      <td>syrian</td>\n",
       "      <td>warn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>investig</td>\n",
       "      <td>movi</td>\n",
       "      <td>busi</td>\n",
       "      <td>evacu</td>\n",
       "      <td>wildlif</td>\n",
       "      <td>parent</td>\n",
       "      <td>care</td>\n",
       "      <td>medal</td>\n",
       "      <td>gym</td>\n",
       "      <td>polici</td>\n",
       "      <td>unemploy</td>\n",
       "      <td>islam</td>\n",
       "      <td>scientist</td>\n",
       "      <td>social</td>\n",
       "      <td>muscl</td>\n",
       "      <td>peac</td>\n",
       "      <td>flood</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>fbi</td>\n",
       "      <td>festiv</td>\n",
       "      <td>invest</td>\n",
       "      <td>road</td>\n",
       "      <td>environ</td>\n",
       "      <td>univers</td>\n",
       "      <td>patient</td>\n",
       "      <td>pet</td>\n",
       "      <td>you</td>\n",
       "      <td>right</td>\n",
       "      <td>employ</td>\n",
       "      <td>pope</td>\n",
       "      <td>use</td>\n",
       "      <td>societi</td>\n",
       "      <td>gym</td>\n",
       "      <td>attack</td>\n",
       "      <td>rain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>convict</td>\n",
       "      <td>fashion</td>\n",
       "      <td>stock</td>\n",
       "      <td>damag</td>\n",
       "      <td>carbon</td>\n",
       "      <td>teach</td>\n",
       "      <td>healthcar</td>\n",
       "      <td>anniversari</td>\n",
       "      <td>board</td>\n",
       "      <td>minist</td>\n",
       "      <td>wage</td>\n",
       "      <td>religion</td>\n",
       "      <td>scientif</td>\n",
       "      <td>discrimin</td>\n",
       "      <td>player</td>\n",
       "      <td>unrest</td>\n",
       "      <td>typhoon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>arrest</td>\n",
       "      <td>media</td>\n",
       "      <td>brand</td>\n",
       "      <td>wildfir</td>\n",
       "      <td>forest</td>\n",
       "      <td>colleg</td>\n",
       "      <td>nurs</td>\n",
       "      <td>kitten</td>\n",
       "      <td>classic</td>\n",
       "      <td>ban</td>\n",
       "      <td>work</td>\n",
       "      <td>franci</td>\n",
       "      <td>engin</td>\n",
       "      <td>racism</td>\n",
       "      <td>leagu</td>\n",
       "      <td>syria</td>\n",
       "      <td>met</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  crime, law and justice arts, culture, entertainment and media  \\\n",
       "0                    law                              entertain   \n",
       "1                sentenc                                 cultur   \n",
       "2                 murder                                   film   \n",
       "3                  polic                                 museum   \n",
       "4                  court                                    art   \n",
       "5               attorney                                 tradit   \n",
       "6               investig                                   movi   \n",
       "7                    fbi                                 festiv   \n",
       "8                convict                                fashion   \n",
       "9                 arrest                                  media   \n",
       "\n",
       "  economy, business and finance disaster, accident and emergency incident  \\\n",
       "0                       compani                                      fire   \n",
       "1                       economi                                     emerg   \n",
       "2                        market                                     crash   \n",
       "3                        econom                                    disast   \n",
       "4                        custom                                     incid   \n",
       "5                       product                                     accid   \n",
       "6                          busi                                     evacu   \n",
       "7                        invest                                      road   \n",
       "8                         stock                                     damag   \n",
       "9                         brand                                   wildfir   \n",
       "\n",
       "   environment education     health human interest lifestyle and leisure  \\\n",
       "0  environment      educ     health       ceremoni                tattoo   \n",
       "1       climat    school     hospit          award               workout   \n",
       "2       pollut   student     diseas          plant               exercis   \n",
       "3       insect     learn  treatment       birthday                garden   \n",
       "4        spill   teacher        nhs            dog                  game   \n",
       "5        speci     cours      medic         celebr                   fit   \n",
       "6      wildlif    parent       care          medal                   gym   \n",
       "7      environ   univers    patient            pet                   you   \n",
       "8       carbon     teach  healthcar    anniversari                 board   \n",
       "9       forest    colleg       nurs         kitten               classic   \n",
       "\n",
       "  politics    labour religion and belief science and technology    society  \\\n",
       "0    polit     retir              religi                 scienc     immigr   \n",
       "1    trump       job              church               research    chariti   \n",
       "2   govern   employe               mosqu                   math        wed   \n",
       "3  amnesti    worker              muslim                 clinic     addict   \n",
       "4    googl     union           christian                   test        fan   \n",
       "5    elect    labour              belief                  space     refuge   \n",
       "6   polici  unemploy               islam              scientist     social   \n",
       "7    right    employ                pope                    use    societi   \n",
       "8   minist      wage            religion               scientif  discrimin   \n",
       "9      ban      work              franci                  engin     racism   \n",
       "\n",
       "          sport conflict, war and peace     weather  \n",
       "0         sport                 protest       storm  \n",
       "1       stadium                     war     weather  \n",
       "2       footbal                militari      cyclon  \n",
       "3        athlet                    coup  temperatur  \n",
       "4     bodybuild               terrorist        snow  \n",
       "5  championship                  syrian        warn  \n",
       "6         muscl                    peac       flood  \n",
       "7           gym                  attack        rain  \n",
       "8        player                  unrest     typhoon  \n",
       "9         leagu                   syria         met  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 10 most important features by category\n",
    "features_df = pd.DataFrame(features)\n",
    "features_df.apply(lambda x: [d['token'] for d in x], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tags(X, labels = None):\n",
    "    '''\n",
    "    Predict tags for a given abstract.\n",
    "    \n",
    "    Args:\n",
    "      - X (list): an iterable with text.\n",
    "      - labels (pandas.Dataframe): label indicators for an abstract\n",
    "    '''\n",
    "    preds = []\n",
    "    if type(X) is str: # convert into iterable if string\n",
    "        X = [X]\n",
    "    \n",
    "    # get prediction from each model\n",
    "    for c in models.keys():\n",
    "        preds.append(models[c].predict(X))\n",
    "    \n",
    "    # print original labels if given\n",
    "    if labels is not None:\n",
    "        assert len(X) == 1, 'Only one extract at a time.'\n",
    "        predicted_tags = [k for k,v in zip(list(models.keys()),preds) if v[0] > 0]\n",
    "        original_tags = list(labels.index[labels.map(lambda x: x>0)])\n",
    "        print('Original Tags: {}'.format(str(original_tags)))\n",
    "        print(\"Predicted Tags: {}\".format(str(predicted_tags)))\n",
    "        \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 1],\n",
       "       [0, 1, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get all predictions\n",
    "y_pred = np.array(predict_tags(X_test['content'])).T\n",
    "\n",
    "# get true labels in the same order\n",
    "y_true = y_test[list(models.keys())].to_numpy()\n",
    "y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Hamming Loss is calculated by taking a fraction of the wrong prediction with the total number of labels. \n",
    "Because Hamming Loss is a loss function, the lower the score is, \n",
    "the better (0 indicates no wrong prediction and 1 indicates all the prediction is wrong)x\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hamming Loss:  0.03\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import hamming_loss\n",
    "print('Hamming Loss: ', round(hamming_loss(y_test, y_pred),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MultiOutputClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from lightgbm import LGBMClassifier  \n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import hamming_loss\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import f1_score, average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MultiOutputClassifier with Logistic Regression\n",
      "Accuracy : 0.4736\n",
      "AUC score: 0.7787947142185324\n",
      "Hamming Loss:  0.05\n"
     ]
    }
   ],
   "source": [
    "print('\\nMultiOutputClassifier with Logistic Regression')\n",
    "pipeline = Pipeline([\n",
    "            (f'tfidf', TfidfVectorizer()),\n",
    "            (f'clf', MultiOutputClassifier(LogisticRegression())),\n",
    "        ])\n",
    "pipeline.fit(X_train['content'], y_train)\n",
    "prediction = pipeline.predict(X_test['content'])\n",
    "\n",
    "\n",
    "accuracy = accuracy_score(y_test, prediction)\n",
    "print(\"Accuracy : {}\".format(round(accuracy,4)))\n",
    "\n",
    "print('AUC score: {}'.format(roc_auc_score(y_test, prediction)))\n",
    "print('Hamming Loss: ', round(hamming_loss(y_test, prediction),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MultiOutputClassifier with Gradient Boosting\n",
      "Accuracy : 0.4533\n",
      "AUC score: 0.8113809975400633\n"
     ]
    }
   ],
   "source": [
    "# 20 mins\n",
    "print('\\nMultiOutputClassifier with Gradient Boosting')\n",
    "pipeline = Pipeline([\n",
    "            (f'tfidf', TfidfVectorizer()),\n",
    "            (f'clf', MultiOutputClassifier(GradientBoostingClassifier())),\n",
    "        ])\n",
    "pipeline.fit(X_train['content'], y_train)\n",
    "prediction = pipeline.predict(X_test['content'])\n",
    "\n",
    "\n",
    "accuracy = accuracy_score(y_test, prediction)\n",
    "print(\"Accuracy : {}\".format(round(accuracy,4)))\n",
    "\n",
    "print('AUC score: {}'.format(roc_auc_score(y_test, prediction)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MultiOutputClassifier with LGBM Classifier\n",
      "Accuracy : 0.562\n",
      "AUC score: 0.867721217987693\n",
      "Hamming Loss:  0.04\n"
     ]
    }
   ],
   "source": [
    "print('\\nMultiOutputClassifier with LGBM Classifier')\n",
    "pipeline = Pipeline([\n",
    "            (f'tfidf', TfidfVectorizer()),\n",
    "            (f'clf', MultiOutputClassifier(LGBMClassifier(learning_rate = 0.08, \n",
    "                                                          num_leaves = 35, \n",
    "                                                          n_estimators = 350, \n",
    "                                                          verbose=-1))),\n",
    "        ])\n",
    "pipeline.fit(X_train['content'], y_train)\n",
    "prediction = pipeline.predict(X_test['content'])\n",
    "\n",
    "accuracy = accuracy_score(y_test, prediction)\n",
    "print(\"Accuracy : {}\".format(round(accuracy,4)))\n",
    "\n",
    "print('AUC score: {}'.format(roc_auc_score(y_test, prediction)))\n",
    "print('Hamming Loss: ', round(hamming_loss(y_test, prediction),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "MultiOutputClassifier with LGBM Classifier\n",
    "Accuracy : 0.562\n",
    "AUC score: 0.8678111608425622\n",
    "Hamming Loss:  0.04\n",
    "learning_rate = 0.08, \n",
    "num_leaves = 35, \n",
    "n_estimators = 350, \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning\n",
    "\n",
    "https://colab.research.google.com/drive/125Q856ee3fKIQcZe0awHFUmyBIMX3PzQ?usp=sharing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
